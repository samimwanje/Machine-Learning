{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Author: Sami Mwanje, mm223kk@student.lnu.se </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy library is used to handle matrixes.\n",
    "\n",
    "Matplotlib to plot and handle the plots of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 3.1: Read data and shuffle the rows in the raw data matrix. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "breastCancer = np.genfromtxt('breast_cancer.csv', delimiter=',')          # Read the breast_cancer.csv file.\n",
    "np.random.shuffle(breastCancer)                                           # Shuffle rows in Python\n",
    "\n",
    " #print(\"X- values: \", breastCancerX)                                       # Print Xs.\n",
    " #print(\"Y-values: \", breastCancerY)                                        # Prints Ys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The needed datas are set here. \n",
    "\n",
    "The breast_cancer.csv file is read using np.genfromtx.\n",
    "\n",
    "The data is shuffled using np.random.shuffle(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 3.2: Replace the responses 2 and 4 with 0 and 1 and divide the dataset into a training set and\n",
    "a test set. How many observations did you allocated for testing, and why this number? </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape X: (546, 9), Shape Y: (546,)\n",
      "Test set shape X: (137, 9), Shape Y: (137,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(len(breastCancer)): \n",
    "    if  breastCancer[i][9] == 2:                                                # Check if current Y is 2. \n",
    "        breastCancer[i][9] = 0                                                  # If yes, change to 0.\n",
    "    else:\n",
    "        breastCancer[i][9] = 1                                                  # If no, change to 1.\n",
    "#print(breastCancerY)                                                           # Print the changed Ys.\n",
    "   \n",
    "\n",
    "trainingSetX = np.array(breastCancer[:546,0:9])\n",
    "trainingSetY = np.array(breastCancer[:546,9])\n",
    "#for i in range(len(trainingSetY)):\n",
    "# print(trainingSetY[i])\n",
    "\n",
    "testSetX = breastCancer[546:,0:9]\n",
    "testSetY = breastCancer[546:,9]\n",
    "\n",
    "print(\"Training set shape X: \" +str(trainingSetX.shape) +\", Shape Y: \" +str(trainingSetY.shape))\n",
    "print(\"Test set shape X: \" +str(testSetX.shape) +\", Shape Y: \" +str(testSetY.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y values are binary labels of either benign (2) or malignant (4), benign is changed to 0 and malignan is changed to 1.\n",
    "\n",
    "The shape for the training set is  (546, 9).\n",
    "\n",
    "The shape for the test set is (137, 9).\n",
    "\n",
    "I chose to use 20 % of the data set for test and 80 % for training.\n",
    "\n",
    "This is to get some variance between training and test data, and\n",
    "\n",
    "at the same time have a large enough test set to compute with with a larger training set so that the alogorithm can be trained with a lot of features and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 3.3: Normalize the training data. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeXData(dataX):\n",
    "    meanValue = dataX.mean(axis=0)                                                  #   Compute mean µi.\n",
    "    stadardDev = dataX.std(axis=0)                                                  #   Compute standard deviation σi.  \n",
    "    return (dataX - meanValue)/stadardDev                                           # Compute normalized Xn in as Xn = (X − µ)/σ. \n",
    "                                                    \n",
    "n = len(trainingSetX)                                                               # X-Rows.      \n",
    "currentData = np.c_[np.ones((n, 1)), normalizeXData(trainingSetX)]                  # Array holding normalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a normalization for the X-data is created. \n",
    "Column contining ones is also added for B0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Train a linear logistic regression model using gradient descent. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  10000\n",
      "Alpha =  0.01\n"
     ]
    }
   ],
   "source": [
    "def g(x):   # Logistic regression function g(M). Sigmoid function.\n",
    "    return (np.exp(x)/(1 + np.exp(x)))\n",
    "\n",
    "N = 10000       # Set hyperparameters N.\n",
    "alpha = 0.01    # Set hyperparameter aplha.\n",
    "\n",
    "costY = []      # Array holding cost for each iteration.\n",
    "def gradientDescent(Xne, Y, Bj, N, alpha):\n",
    "    for i in range(N):  # Compute N times.\n",
    "        Bj = Bj - (np.dot(((alpha/n)*Xne.T), ((g(np.dot(Xne,Bj)))-Y))) # Compute gradient descent logisic regression  βj+1 = βj −(α/n)XT(g(Xβ) − y)\n",
    "        cost = (-1/n) * (np.dot(Y.T, np.log(g(np.dot(Xne,Bj))))+np.dot((1-Y).T, np.log(1-g(np.dot(Xne,Bj)))))   # Calculate cost function\n",
    "        costY.append(cost)  # Add cost to costY list.\n",
    "\n",
    "    return Bj          # Return computed Bj.\n",
    "\n",
    "Bj = np.zeros(len(currentData[0]))  # Set gradient descent start point as β = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "Bgradient = gradientDescent(currentData, trainingSetY, Bj, N, alpha)       # Recieve B, created using gradient descent.\n",
    "print(\"N = \", N)                            # Printing N that gave a 1 % margin.\n",
    "print(\"Alpha = \", alpha)                    # Printing the alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here gradient descent is runing using the model and logistic regression.\n",
    "\n",
    "The gradient descent iterates for 10000 times with a rate of 0.01\n",
    "\n",
    "The cost function is also calculated and added in an array which will be used for the print."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Print the hyperparameters α and Niter and plot the cost function J(β) as a\n",
    "function over iterations.  </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, N + 1), costY)\n",
    "plt.xlabel('N - iterations')\n",
    "plt.ylabel('Cost J(β)')\n",
    "plt.title(\"Cost function J(β)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 3.4: What is the training error (number of non-correct classifications in the training data) and\n",
    "the training accuracy (percentage of correct classifications) for your model? </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set errors: 15\n",
      "Training set accuracy: 97.25 %\n"
     ]
    }
   ],
   "source": [
    "predictedY = g(np.dot(currentData ,Bgradient))  # Predict all values using the beta from the gradient descent.\n",
    "\n",
    "errors = 0   # Variable counting wrong predictions.\n",
    "correct = 0 # Variable counting correct predictions.\n",
    "for i in range(len(predictedY)):\n",
    "    if  (predictedY[i] >= 0.5) == (trainingSetY[i]==1): # Check if the current prediction is correct.\n",
    "        correct += 1                                    # If correct add 1 to correct.\n",
    "    else:\n",
    "        errors +=1                                       # If wrong add 1 to wrong.\n",
    "\n",
    "print(\"Training set errors: \" +str(errors))\n",
    "print(\"Training set accuracy: \" +str( round(100 *(correct/len(predictedY)),2)) +\" %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set gets around 12 to 18 errors for each computation.\n",
    "\n",
    "That is an accuracy around 97+ % for the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 3.5: What is the number of test error and the test accuracy for your model? </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set errors: 3\n",
      "Test set accuracy: 97.81 %\n"
     ]
    }
   ],
   "source": [
    "n = len(testSetX)                                                               # X-Rows.      \n",
    "currentData = np.c_[np.ones((n, 1)), normalizeXData(testSetX)]                  # Array holding normalized data.\n",
    "\n",
    "Bj = np.zeros(len(currentData[0]))  # Set gradient descent start point as β = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "Bgradient = gradientDescent(currentData, testSetY, Bj, N, alpha)       # Recieve B, created using gradient descent.\n",
    "predictedY = g(np.dot(currentData ,Bgradient))  # Predict all values using the beta from the gradient descent.\n",
    "\n",
    "errors = 0   # Variable counting wrong predictions.\n",
    "correct = 0 # Variable counting correct predictions.\n",
    "for i in range(len(predictedY)):\n",
    "    if  (predictedY[i] >= 0.5) == (testSetY[i]==1): # Check if the current prediction is correct.\n",
    "        correct += 1                                    # If correct add 1 to correct.\n",
    "    else:\n",
    "        errors +=1                                       # If wrong add 1 to wrong.\n",
    "\n",
    "print(\"Test set errors: \" +str(errors))\n",
    "print(\"Test set accuracy: \" +str( round(100 *(correct/len(predictedY)),2)) +\" %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set gets around 0 to 7 errors for each computation.\n",
    "\n",
    "That is an accuracy around 96+ % for the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Exercise 3.6: Repeated runs will (due to the shuffling) give different results. Are they qualitatively\n",
    "the same? Do they depend on how many observations you put aside for testing? Is the\n",
    "difference between training and testing expected?\n",
    " </b>\n",
    "\n",
    "The training set is stuck at an accuracy of 97 %, no matter how many times i repeat the run the training set is always around 97 %.\n",
    "\n",
    "The test set though can vary between 0 errors to 6 error, due to the shuffle round. A \"good\" shuffle round can give up to 100 % accuracy for the test set,\n",
    "\n",
    "and a \"bad\" one can give an error around 95 %.\n",
    "\n",
    "The changes above are mainly because of the shuffle, the shuffle trains  the models with new training and test data, which effects the accuracy.\n",
    "\n",
    "Since this is a pretty good model and algorithm changing the size of the test and training data does not largely affect the accuracy.\n",
    "\n",
    "Though a larger test set gives more errors in the test model, while there are less training erros.\n",
    "\n",
    "Smaller test data, gives less errors but the accuracy maintains. The training data will here have more room for errors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e551358aee86a69aef693f7da0ddb4f4dd586d41867e06372ce040678483a8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
